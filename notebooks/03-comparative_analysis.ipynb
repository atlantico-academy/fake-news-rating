{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4885fd03-686f-492c-92bd-c6a363b9d3bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8afd6c8-6bbf-4702-866f-f78c76ae80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV,cross_val_score,KFold,ShuffleSplit,cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report,roc_auc_score,roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import  Markdown\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8508683-6a38-43a8-918a-4660d69a3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6ccc9-722b-41da-890e-99f07b94a6a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc75079-0452-4c7c-89f9-fc4ec2fca6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/interim/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7904bcf-2a8c-4940-a716-5314db8084d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_score_v2(data_X,data_Y,data_name,estimator,estimator_name,estimator_params):\n",
    "    scores = [\"recall\",\"accuracy\",\"f1\"]\n",
    "    print(\"data name: \", data_name)\n",
    "    \n",
    "    print(\"model name: \",estimator_name)\n",
    "    \n",
    "    model_gs = BayesSearchCV(estimator, search_spaces=estimator_params, scoring='accuracy')\n",
    "\n",
    "    results = cross_validate(model_gs,data_X,data_Y, scoring=scores, cv=ShuffleSplit(n_splits=3, test_size=0.2, random_state=42))\n",
    "    results[\"model_name\"] = [f\"{estimator_name}-{data_name}\"] * len(results[\"score_time\"])\n",
    "    \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e9ec47-a785-4689-a3ac-7d20a87785f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modificar\n",
    "def build_score(y_pred,y_true,probs):\n",
    "    print(\"Acuracia : {}%\".format(round(accuracy_score(y_pred=y_pred,y_true=y_true)*100,3)))\n",
    "    print(\"F1_score: {}%\".format(round(f1_score(y_pred=y_pred,y_true=y_true)*100,3)))\n",
    "    print(\"Precison: {}%\".format(round(precision_score(y_pred=y_pred,y_true=y_true)*100,3)))\n",
    "    print(\"Recall: {}%\".format(round(recall_score(y_pred=y_pred,y_true=y_true)*100,3)))\n",
    "    print(\"-\"*20)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test,y_pred,target_names=[\"FAKE\",\"TRUE\"]))\n",
    "    print(\"-\"*20)\n",
    "    print(\"Plot curva roc\")\n",
    "    lr_auc = roc_auc_score(y_true, probs[:, 1])\n",
    "    print('ROC AUC=%.3f' % (lr_auc))\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs[:, 1])\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24373749-3d36-49c6-b343-8f84a48ba624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s, props=''):\n",
    "    values = [float(value.split()[0]) for value in s.values[1:]]\n",
    "    result = [''] * len(s.values)\n",
    "    if s.values[0].endswith('time'):\n",
    "        result[np.argmin(values)+1] = props\n",
    "    else:\n",
    "        result[np.argmax(values)+1] = props\n",
    "    return result\n",
    "\n",
    "def get_winner(s):\n",
    "    metric = s.values[0]\n",
    "    values = [float(value.split()[0]) for value in s.values[1:]]\n",
    "    models = results.columns[1:]\n",
    "    \n",
    "    if s.values[0].endswith('time'):\n",
    "        return models[np.argmin(values)]\n",
    "    else:\n",
    "        return models[np.argmax(values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b16e93-6185-4162-9dea-7f7d82cabd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (\n",
    "        \"RF\",\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            \"max_depth\": [5, 8, 15, 25],\n",
    "            \"max_features\":['log2', 'sqrt']\n",
    "        }\n",
    "    ),\n",
    "    ( \n",
    "        \"LR\",\n",
    "        LogisticRegression(solver='liblinear', max_iter=10000),\n",
    "        {\n",
    "            \"penalty\": ['l1', 'l2'],\n",
    "            \"C\":[0.001,0.01,0.1,1]\n",
    "        }\n",
    "    ),\n",
    "     (\n",
    "         \"Tree\",\n",
    "          DecisionTreeClassifier(),\n",
    "         {\n",
    "            'criterion' : ['gini', 'entropy'],\n",
    "            'max_depth' : [6,8,10,12],\n",
    "            'max_features':[2,4,6,8]\n",
    "\n",
    "         }\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249489f1-0d45-4333-bb18-cc1c917b1ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10c07e79-42a2-4d0a-8742-914faf60657e",
   "metadata": {},
   "source": [
    "Rodar e guardar o resultado de cada dado separado, modificar amanhã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26321a7f-83a7-4fb6-aa8e-ab8bf5eb8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # função para comprar os modelos\n",
    "# def compara_modelos(datasets:list, estimators:list):\n",
    "#     # compara os modelos por dataset, já tunando os hyperparametros compara o melhor com melhor\n",
    "#     results={}\n",
    "#     for data_name,data_path in datasets:\n",
    "#         vetorizar = TfidfVectorizer(lowercase=False, max_features=200)#mex_features -> 300\n",
    "#         df = pd.read_csv(data_path)\n",
    "#         # dividir dataset para validar o modelo após o treino.\n",
    "#         df_train = df.sample(n=6000)\n",
    "#         bag_of_words_ = vetorizar.fit_transform(df_train[\"text\"])\n",
    "#         X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "#         y = df_train[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "#         for estimator_name, estimator_obj, estimator_params in estimators:\n",
    "\n",
    "#             model_results = cross_score_v2(X,y,data_name,estimator_obj,estimator_name,estimator_params)\n",
    "#             if results:\n",
    "#                 for key, value in model_results.items():\n",
    "#                     results[key] = np.append(results[key], value)\n",
    "#             else:\n",
    "#                 results = model_results\n",
    "                \n",
    "                \n",
    "#     #guarda o resultado da comparação\n",
    "#     df_results = pd.DataFrame(results)\n",
    "#     results = (\n",
    "#         pd\n",
    "#         .DataFrame(df_results)\n",
    "#         .groupby(['model_name'])\n",
    "#         .agg([lambda x: f\"{np.mean(x):.3f} ± {np.std(x):.3f}\"])\n",
    "#         .transpose()\n",
    "#         .reset_index()\n",
    "#         .rename(columns={\"level_0\": \"score\"})\n",
    "#         .drop(columns=\"level_1\")\n",
    "#             # .set_index('score')\n",
    "#     )\n",
    "#     # estiliza o dataframe deixando em cinza o melhor modelo\n",
    "#     time_scores = ['fit_time', 'score_time']\n",
    "#     winner = results.query('score not in @time_scores').apply(get_winner, axis=1).value_counts().index[0]\n",
    "#     results.columns.name = ''\n",
    "#     results = (\n",
    "#         results\n",
    "#         .style\n",
    "#         .hide(axis='index')\n",
    "#         .apply(highlight_max, props='color:white;background-color:gray', axis=1)\n",
    "#     )\n",
    "#     display(results)\n",
    "#     display(Markdown(f'O melhor modelo é o : **{winner}**'))\n",
    "#     # escolhe o melhor modelo\n",
    "#     model_winner = winner.split(\"-\")[0]\n",
    "#     data_winner= winner.split(\"-\")[1]\n",
    "#     model_name, model, model_params  = [foo for foo in estimators if foo[0] == model_winner][0]\n",
    "#     data_name, data_path  = [foo for foo in datasets if foo[0] == data_winner][0]\n",
    "    \n",
    "#     # treina o melhor modelo com todos os textos\n",
    "#     vetorizar = TfidfVectorizer(lowercase=False, max_features=200)\n",
    "#     df = pd.read_csv(data_path)\n",
    "#     df_train = df.sample(n=6000)\n",
    "#     bag_of_words_ = vetorizar.fit_transform(df_train[\"text\"])\n",
    "#     X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "#     y = df_train[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "    \n",
    "#     # tuna os hyperparametros do melhor modelo\n",
    "#     model_BS = BayesSearchCV(model, search_spaces=model_params, scoring='accuracy')\n",
    "#     model_BS.fit(X, y)\n",
    "    \n",
    "#     # Salvando modelo treinado\n",
    "#     path =  f\"../models/model_{model.__class__.__name__}_{data_name}.joblib\"\n",
    "#     joblib.dump(model_BS,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718a741-d900-4408-928a-756816367c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16cdfad9-e6e7-4b0a-954b-e6520d02e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"no_stop_words_lemma\",\"../data/interim/no_stopWords_lemma.csv\"),\n",
    "    (\"no_stop_words_stemma\",\"../data/interim/no_stopWords_stemma.csv\"),\n",
    "    (\"with_stop_words_lemma\",\"../data/interim/with_stopWords_lemma.csv\"),\n",
    "    (\"with_stop_words_stemma\",\"../data/interim/with_stopWords_stemma.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3436f8ac-f0a1-49cf-80f7-1a11eb6575c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compara_modelos(datasets_no_stop, estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d864ee-dcc0-4b73-a11c-aefa71c3466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compara_modelos(datasets_with_Stop, estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe5ed9-c614-48f2-a7e8-73f09eb7439c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059edeaf-3d93-464e-af57-e126df9fe654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb78b2-9f7b-4711-9130-a5706a4e48ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5011365-9c41-4d3c-8e5b-cc508d503a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf497a6-3e5c-4030-b88c-8015788f2d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e26c5e-c7bf-4c27-9d20-1580ed15b70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60ca7f-2466-4bea-9008-c3a519671ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38760faa-b079-4791-8271-8cc56a8a77fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeb0ce1-0fdf-4b5c-af32-8ca7ff8d8831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data name:  no_stop_words_lemma\n",
      "model name:  RF\n",
      "data name:  no_stop_words_lemma\n",
      "model name:  LR\n",
      "data name:  no_stop_words_lemma\n",
      "model name:  Tree\n",
      "data name:  no_stop_words_stemma\n",
      "model name:  RF\n",
      "data name:  no_stop_words_stemma\n",
      "model name:  LR\n",
      "data name:  no_stop_words_stemma\n",
      "model name:  Tree\n",
      "data name:  with_stop_words_lemma\n",
      "model name:  RF\n",
      "data name:  with_stop_words_lemma\n",
      "model name:  LR\n",
      "data name:  with_stop_words_lemma\n",
      "model name:  Tree\n",
      "data name:  with_stop_words_stemma\n",
      "model name:  RF\n",
      "data name:  with_stop_words_stemma\n",
      "model name:  LR\n",
      "data name:  with_stop_words_stemma\n",
      "model name:  Tree\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_77eab_row0_col12, #T_77eab_row1_col11, #T_77eab_row2_col7, #T_77eab_row3_col8, #T_77eab_row4_col8 {\n",
       "  color: white;\n",
       "  background-color: gray;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_77eab\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_77eab_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n",
       "      <th id=\"T_77eab_level0_col1\" class=\"col_heading level0 col1\" >LR-no_stop_words_lemma</th>\n",
       "      <th id=\"T_77eab_level0_col2\" class=\"col_heading level0 col2\" >LR-no_stop_words_stemma</th>\n",
       "      <th id=\"T_77eab_level0_col3\" class=\"col_heading level0 col3\" >LR-with_stop_words_lemma</th>\n",
       "      <th id=\"T_77eab_level0_col4\" class=\"col_heading level0 col4\" >LR-with_stop_words_stemma</th>\n",
       "      <th id=\"T_77eab_level0_col5\" class=\"col_heading level0 col5\" >RF-no_stop_words_lemma</th>\n",
       "      <th id=\"T_77eab_level0_col6\" class=\"col_heading level0 col6\" >RF-no_stop_words_stemma</th>\n",
       "      <th id=\"T_77eab_level0_col7\" class=\"col_heading level0 col7\" >RF-with_stop_words_lemma</th>\n",
       "      <th id=\"T_77eab_level0_col8\" class=\"col_heading level0 col8\" >RF-with_stop_words_stemma</th>\n",
       "      <th id=\"T_77eab_level0_col9\" class=\"col_heading level0 col9\" >Tree-no_stop_words_lemma</th>\n",
       "      <th id=\"T_77eab_level0_col10\" class=\"col_heading level0 col10\" >Tree-no_stop_words_stemma</th>\n",
       "      <th id=\"T_77eab_level0_col11\" class=\"col_heading level0 col11\" >Tree-with_stop_words_lemma</th>\n",
       "      <th id=\"T_77eab_level0_col12\" class=\"col_heading level0 col12\" >Tree-with_stop_words_stemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_77eab_row0_col0\" class=\"data row0 col0\" >fit_time</td>\n",
       "      <td id=\"T_77eab_row0_col1\" class=\"data row0 col1\" >98.596 ± 2.924</td>\n",
       "      <td id=\"T_77eab_row0_col2\" class=\"data row0 col2\" >115.698 ± 9.819</td>\n",
       "      <td id=\"T_77eab_row0_col3\" class=\"data row0 col3\" >211.645 ± 7.145</td>\n",
       "      <td id=\"T_77eab_row0_col4\" class=\"data row0 col4\" >131.707 ± 8.549</td>\n",
       "      <td id=\"T_77eab_row0_col5\" class=\"data row0 col5\" >309.997 ± 8.729</td>\n",
       "      <td id=\"T_77eab_row0_col6\" class=\"data row0 col6\" >311.079 ± 11.182</td>\n",
       "      <td id=\"T_77eab_row0_col7\" class=\"data row0 col7\" >374.757 ± 1.076</td>\n",
       "      <td id=\"T_77eab_row0_col8\" class=\"data row0 col8\" >311.020 ± 11.178</td>\n",
       "      <td id=\"T_77eab_row0_col9\" class=\"data row0 col9\" >109.047 ± 5.298</td>\n",
       "      <td id=\"T_77eab_row0_col10\" class=\"data row0 col10\" >122.243 ± 4.629</td>\n",
       "      <td id=\"T_77eab_row0_col11\" class=\"data row0 col11\" >88.335 ± 1.840</td>\n",
       "      <td id=\"T_77eab_row0_col12\" class=\"data row0 col12\" >79.566 ± 2.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_77eab_row1_col0\" class=\"data row1 col0\" >score_time</td>\n",
       "      <td id=\"T_77eab_row1_col1\" class=\"data row1 col1\" >0.011 ± 0.001</td>\n",
       "      <td id=\"T_77eab_row1_col2\" class=\"data row1 col2\" >0.009 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row1_col3\" class=\"data row1 col3\" >0.007 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row1_col4\" class=\"data row1 col4\" >0.006 ± 0.000</td>\n",
       "      <td id=\"T_77eab_row1_col5\" class=\"data row1 col5\" >0.034 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row1_col6\" class=\"data row1 col6\" >0.029 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row1_col7\" class=\"data row1 col7\" >0.027 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row1_col8\" class=\"data row1 col8\" >0.022 ± 0.001</td>\n",
       "      <td id=\"T_77eab_row1_col9\" class=\"data row1 col9\" >0.005 ± 0.000</td>\n",
       "      <td id=\"T_77eab_row1_col10\" class=\"data row1 col10\" >0.005 ± 0.000</td>\n",
       "      <td id=\"T_77eab_row1_col11\" class=\"data row1 col11\" >0.004 ± 0.000</td>\n",
       "      <td id=\"T_77eab_row1_col12\" class=\"data row1 col12\" >0.005 ± 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_77eab_row2_col0\" class=\"data row2 col0\" >test_recall</td>\n",
       "      <td id=\"T_77eab_row2_col1\" class=\"data row2 col1\" >0.926 ± 0.008</td>\n",
       "      <td id=\"T_77eab_row2_col2\" class=\"data row2 col2\" >0.930 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row2_col3\" class=\"data row2 col3\" >0.943 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row2_col4\" class=\"data row2 col4\" >0.946 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row2_col5\" class=\"data row2 col5\" >0.944 ± 0.003</td>\n",
       "      <td id=\"T_77eab_row2_col6\" class=\"data row2 col6\" >0.944 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row2_col7\" class=\"data row2 col7\" >0.950 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row2_col8\" class=\"data row2 col8\" >0.949 ± 0.008</td>\n",
       "      <td id=\"T_77eab_row2_col9\" class=\"data row2 col9\" >0.845 ± 0.017</td>\n",
       "      <td id=\"T_77eab_row2_col10\" class=\"data row2 col10\" >0.846 ± 0.007</td>\n",
       "      <td id=\"T_77eab_row2_col11\" class=\"data row2 col11\" >0.847 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row2_col12\" class=\"data row2 col12\" >0.833 ± 0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_77eab_row3_col0\" class=\"data row3 col0\" >test_accuracy</td>\n",
       "      <td id=\"T_77eab_row3_col1\" class=\"data row3 col1\" >0.934 ± 0.006</td>\n",
       "      <td id=\"T_77eab_row3_col2\" class=\"data row3 col2\" >0.934 ± 0.006</td>\n",
       "      <td id=\"T_77eab_row3_col3\" class=\"data row3 col3\" >0.920 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row3_col4\" class=\"data row3 col4\" >0.938 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row3_col5\" class=\"data row3 col5\" >0.952 ± 0.002</td>\n",
       "      <td id=\"T_77eab_row3_col6\" class=\"data row3 col6\" >0.951 ± 0.003</td>\n",
       "      <td id=\"T_77eab_row3_col7\" class=\"data row3 col7\" >0.950 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row3_col8\" class=\"data row3 col8\" >0.954 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row3_col9\" class=\"data row3 col9\" >0.860 ± 0.011</td>\n",
       "      <td id=\"T_77eab_row3_col10\" class=\"data row3 col10\" >0.862 ± 0.003</td>\n",
       "      <td id=\"T_77eab_row3_col11\" class=\"data row3 col11\" >0.867 ± 0.012</td>\n",
       "      <td id=\"T_77eab_row3_col12\" class=\"data row3 col12\" >0.868 ± 0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_77eab_row4_col0\" class=\"data row4 col0\" >test_f1</td>\n",
       "      <td id=\"T_77eab_row4_col1\" class=\"data row4 col1\" >0.934 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row4_col2\" class=\"data row4 col2\" >0.935 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row4_col3\" class=\"data row4 col3\" >0.924 ± 0.006</td>\n",
       "      <td id=\"T_77eab_row4_col4\" class=\"data row4 col4\" >0.939 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row4_col5\" class=\"data row4 col5\" >0.952 ± 0.003</td>\n",
       "      <td id=\"T_77eab_row4_col6\" class=\"data row4 col6\" >0.952 ± 0.003</td>\n",
       "      <td id=\"T_77eab_row4_col7\" class=\"data row4 col7\" >0.951 ± 0.004</td>\n",
       "      <td id=\"T_77eab_row4_col8\" class=\"data row4 col8\" >0.954 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row4_col9\" class=\"data row4 col9\" >0.859 ± 0.009</td>\n",
       "      <td id=\"T_77eab_row4_col10\" class=\"data row4 col10\" >0.863 ± 0.005</td>\n",
       "      <td id=\"T_77eab_row4_col11\" class=\"data row4 col11\" >0.867 ± 0.013</td>\n",
       "      <td id=\"T_77eab_row4_col12\" class=\"data row4 col12\" >0.864 ± 0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f871023e190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "O melhor modelo é o : **RF-with_stop_words_stemma**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['../models/model_RandomForestClassifier_with_stop_words_stemma.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results={}\n",
    "for data_name,data_path in datasets:\n",
    "    vetorizar = TfidfVectorizer(lowercase=False, max_features=200)#mex_features -> 300\n",
    "    df = pd.read_csv(data_path)\n",
    "    # separando uma parte para  teste\n",
    "    df_train = df.sample(n=6000, random_state=42)\n",
    "    bag_of_words_ = vetorizar.fit_transform(df_train[\"text\"])\n",
    "    X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "    y = df_train[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "    for estimator_name, estimator_obj, estimator_params in estimators:\n",
    "\n",
    "        model_results = cross_score_v2(X,y,data_name,estimator_obj,estimator_name,estimator_params)\n",
    "        if results:\n",
    "            for key, value in model_results.items():\n",
    "                results[key] = np.append(results[key], value)\n",
    "        else:\n",
    "            results = model_results\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "results = (\n",
    "    pd\n",
    "    .DataFrame(df_results)\n",
    "    .groupby(['model_name'])\n",
    "    .agg([lambda x: f\"{np.mean(x):.3f} ± {np.std(x):.3f}\"])#\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"score\"})\n",
    "    .drop(columns=\"level_1\")\n",
    "    # .set_index('score')\n",
    ")\n",
    "time_scores = ['fit_time', 'score_time']\n",
    "winner = results.query('score not in @time_scores').apply(get_winner, axis=1).value_counts().index[0]\n",
    "results.columns.name = ''\n",
    "results = (\n",
    "    results\n",
    "    .style\n",
    "    .hide(axis='index')\n",
    "    .apply(highlight_max, props='color:white;background-color:gray', axis=1)\n",
    ")\n",
    "display(results)\n",
    "display(Markdown(f'O melhor modelo é o : **{winner}**'))\n",
    "\n",
    "\n",
    "\n",
    "# Realizando treino do modelo completo\n",
    "model_winner = winner.split(\"-\")[0]\n",
    "data_winner= winner.split(\"-\")[1]\n",
    "model_name, model, model_params  = [foo for foo in estimators if foo[0] == model_winner][0]\n",
    "data_name, data_path  = [foo for foo in datasets if foo[0] == data_winner][0]\n",
    "\n",
    "vetorizar = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "df = pd.read_csv(data_path).sample(frac=1, random_state=42)\n",
    "#df_train = df.sample(n=6000)\n",
    "bag_of_words_ = vetorizar.fit_transform(df[\"text\"])\n",
    "X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "y = df[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "\n",
    "model_Bs = BayesSearchCV(model, search_spaces=model_params, scoring='accuracy')\n",
    "model_Bs.fit(X, y)\n",
    "\n",
    "\n",
    "# Salvando modelo treinado\n",
    "path =  f\"../models/model_{model.__class__.__name__}_{data_name}.joblib\"\n",
    "joblib.dump(model_Bs,path)\n",
    "# Salvando o preprocessor\n",
    "path = f\"../models/preprocessor.joblib\"\n",
    "joblib.dump(bag_of_words_, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb8a832e-7569-44b7-8ff5-07b0d6414add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>chor as pitang bonn post fras de dor de cotove...</td>\n",
       "      <td>tv_celebridades</td>\n",
       "      <td>fake</td>\n",
       "      <td>https://www.diariodobrasil.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>na bah adolesc de 14 ano prev a própr mort e a...</td>\n",
       "      <td>tv_celebridades</td>\n",
       "      <td>fake</td>\n",
       "      <td>https://www.diariodobrasil.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>segundaf 12 de març bom dia aqu est os princip...</td>\n",
       "      <td>sociedade_cotidiano</td>\n",
       "      <td>true</td>\n",
       "      <td>https://g1.globo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>a insan de lul se me prend eu vir heró se me d...</td>\n",
       "      <td>politica</td>\n",
       "      <td>fake</td>\n",
       "      <td>https://www.diariodobrasil.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>crianç apanh de mulh petist só porqu us uma ca...</td>\n",
       "      <td>politica</td>\n",
       "      <td>fake</td>\n",
       "      <td>https://www.diariodobrasil.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>segundaf 11 de setembr de 2017 boa noit aqu es...</td>\n",
       "      <td>politica</td>\n",
       "      <td>true</td>\n",
       "      <td>https://g1.globo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>rodrig fonsec quas um ano após sua estre no ci...</td>\n",
       "      <td>tv_celebridades</td>\n",
       "      <td>true</td>\n",
       "      <td>http://cultura.estadao.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>quartaf 6 de setembr de 2017 boa noit aqu est ...</td>\n",
       "      <td>politica</td>\n",
       "      <td>true</td>\n",
       "      <td>https://g1.globo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>manifest mantêm protest em charlott após políc...</td>\n",
       "      <td>sociedade_cotidiano</td>\n",
       "      <td>true</td>\n",
       "      <td>http://internacional.estadao.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>red soc suspend cont de rapp americ xenófob el...</td>\n",
       "      <td>tv_celebridades</td>\n",
       "      <td>fake</td>\n",
       "      <td>https://www.diariodobrasil.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text             category  \\\n",
       "3098  chor as pitang bonn post fras de dor de cotove...      tv_celebridades   \n",
       "2531  na bah adolesc de 14 ano prev a própr mort e a...      tv_celebridades   \n",
       "4071  segundaf 12 de març bom dia aqu est os princip...  sociedade_cotidiano   \n",
       "1287  a insan de lul se me prend eu vir heró se me d...             politica   \n",
       "2540  crianç apanh de mulh petist só porqu us uma ca...             politica   \n",
       "...                                                 ...                  ...   \n",
       "3772  segundaf 11 de setembr de 2017 boa noit aqu es...             politica   \n",
       "5191  rodrig fonsec quas um ano após sua estre no ci...      tv_celebridades   \n",
       "5226  quartaf 6 de setembr de 2017 boa noit aqu est ...             politica   \n",
       "5390  manifest mantêm protest em charlott após políc...  sociedade_cotidiano   \n",
       "860   red soc suspend cont de rapp americ xenófob el...      tv_celebridades   \n",
       "\n",
       "     label                                 site  \n",
       "3098  fake       https://www.diariodobrasil.org  \n",
       "2531  fake       https://www.diariodobrasil.org  \n",
       "4071  true                 https://g1.globo.com  \n",
       "1287  fake       https://www.diariodobrasil.org  \n",
       "2540  fake       https://www.diariodobrasil.org  \n",
       "...    ...                                  ...  \n",
       "3772  true                 https://g1.globo.com  \n",
       "5191  true        http://cultura.estadao.com.br  \n",
       "5226  true                 https://g1.globo.com  \n",
       "5390  true  http://internacional.estadao.com.br  \n",
       "860   fake       https://www.diariodobrasil.org  \n",
       "\n",
       "[7200 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b46c48-dfdb-477a-9a72-0c59fa4af3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/preprocessor.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# winner = 'RF-with_stop_words_stemma'\n",
    "# # Realizando treino do modelo completo\n",
    "# model_winner = winner.split(\"-\")[0]\n",
    "# data_winner= winner.split(\"-\")[1]\n",
    "# model_name, model, model_params  = [foo for foo in estimators if foo[0] == model_winner][0]\n",
    "# data_name, data_path  = [foo for foo in datasets if foo[0] == data_winner][0]\n",
    "\n",
    "# vetorizar = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "# df = pd.read_csv(data_path).sample(frac=1, random_state=42)\n",
    "# #df_train = df.sample(n=6000)\n",
    "# bag_of_words_ = vetorizar.fit_transform(df[\"text\"])\n",
    "# X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "# y = df[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "\n",
    "# model_Bs = BayesSearchCV(model, search_spaces=model_params, scoring='accuracy')\n",
    "# model_Bs.fit(X, y)\n",
    "\n",
    "\n",
    "# # Salvando modelo treinado\n",
    "# path =  f\"../models/model_{model.__class__.__name__}_{data_name}.joblib\"\n",
    "# joblib.dump(model_Bs,path)\n",
    "# # Salvando o preprocessor\n",
    "# path = f\"../models/preprocessor.joblib\"\n",
    "# joblib.dump(vetorizar, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4eade1-636b-4848-8393-10f3e17ec3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         1.26803651 1.         1.\n",
      " 1.         1.         1.         1.         1.26803651 1.        ]\n",
      "\n",
      "[1.         1.         1.         1.22693474 1.         1.\n",
      " 1.         1.         1.         1.         1.22693474 1.        ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from scipy.spatial.distance import cdist\n",
    "# # df\n",
    "\n",
    "# text_vet = vetorizar.transform(\"\"\"Luiz Inácio Lula da Silva (PT) é o novo presidente da República.\"\"\".split(' '))\n",
    "\n",
    "# dist_fake = cdist(text_vet.todense(), X[y==0]).mean(axis=1)\n",
    "# dist_True = cdist(text_vet.todense(), X[y==1]).mean(axis=1)\n",
    "\n",
    "# print(dist_fake)\n",
    "# print('')\n",
    "# print(dist_True)\n",
    "# print('')\n",
    "# text_vet.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c82bb114-f235-41ef-abc6-1a8f8cb721e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/preprocessor.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = f\"../models/preprocessor.joblib\"\n",
    "\n",
    "# joblib.dump(vetorizar, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4326e5-ec4e-4e42-a7bb-3c22316a83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results={}\n",
    "# for data_name,data_path in datasets_with_Stop:\n",
    "#     vetorizar = TfidfVectorizer(lowercase=False, max_features=200)#mex_features -> 300\n",
    "#     df = pd.read_csv(data_path)\n",
    "#     df_train = df.sample(n=6000)\n",
    "#     bag_of_words_ = vetorizar.fit_transform(df_train[\"text\"])\n",
    "#     X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "#     y = df_train[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "#     for estimator_name, estimator_obj, estimator_params in estimators:\n",
    "\n",
    "#         model_results = cross_score_v2(X,y,data_name,estimator_obj,estimator_name,estimator_params)\n",
    "#         if results:\n",
    "#             for key, value in model_results.items():\n",
    "#                 results[key] = np.append(results[key], value)\n",
    "#         else:\n",
    "#             results = model_results\n",
    "\n",
    "\n",
    "# df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# results = (\n",
    "#     pd\n",
    "#     .DataFrame(df_results)\n",
    "#     .groupby(['model_name'])\n",
    "#     .agg([lambda x: f\"{np.mean(x):.3f} ± {np.std(x):.3f}\"])#\n",
    "#     .transpose()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={\"level_0\": \"score\"})\n",
    "#     .drop(columns=\"level_1\")\n",
    "#     # .set_index('score')\n",
    "# )\n",
    "# time_scores = ['fit_time', 'score_time']\n",
    "# winner = results.query('score not in @time_scores').apply(get_winner, axis=1).value_counts().index[0]\n",
    "# results.columns.name = ''\n",
    "# results = (\n",
    "#     results\n",
    "#     .style\n",
    "#     .hide(axis='index')\n",
    "#     .apply(highlight_max, props='color:white;background-color:gray', axis=1)\n",
    "# )\n",
    "# display(results)\n",
    "# display(Markdown(f'O melhor modelo é o : **{winner}**'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69516d91-11af-448a-a6d1-89728a22c97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/model_RandomForestClassifier_with_stop_words_lemma.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando treino do modelo completo\n",
    "# model_winner = winner.split(\"-\")[0]\n",
    "# data_winner= winner.split(\"-\")[1]\n",
    "# model_name, model, model_params  = [foo for foo in estimators if foo[0] == model_winner][0]\n",
    "# data_name, data_path  = [foo for foo in datasets_with_Stop if foo[0] == data_winner][0]\n",
    "\n",
    "# vetorizar = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "# df = pd.read_csv(data_path)\n",
    "# bag_of_words_ = vetorizar.fit_transform(df[\"text\"])\n",
    "# X = pd.DataFrame(bag_of_words_.toarray(),columns=vetorizar.get_feature_names_out())\n",
    "# y = df[\"label\"].replace({\"true\":1,\"fake\":0}).to_numpy().ravel()\n",
    "\n",
    "# model_BS =  BayesSearchCV(model, search_spaces=model_params, scoring='accuracy')\n",
    "# model_BS.fit(X, y)\n",
    "\n",
    "\n",
    "# # Salvando modelo treinado\n",
    "# path =  f\"../models/model_{model.__class__.__name__}_{data_name}.joblib\"\n",
    "# joblib.dump(model_BS,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ca7d1-7a3d-44ab-945a-a759d890dd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
